{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## By Allen Huang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /anaconda3/lib/python3.7/site-packages (0.4.2)\r\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from torchvision) (1.16.4)\r\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from torchvision) (1.12.0)\r\n",
      "Requirement already satisfied: torch in /anaconda3/lib/python3.7/site-packages (from torchvision) (1.3.1)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /anaconda3/lib/python3.7/site-packages (from torchvision) (6.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "# use data from Pytorch's \"torchvision\" \n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first thing we have to consider is our data. In most tutorials, this bit is often overlooked in the interest of going straight to the training of a neural network. That said, as a programmer working with neural networks, one of your largest tasks is preprocessing your data and formatting it in such as way to be easiest for the neural network to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now though, we're just trying to learn about how to do a basic neural network in pytorch, so we'll use torchvision here, to load the MNIST dataset, which is a image-based dataset showing handwritten digits from 0-9, and your job is to write a neural network to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9912320/9912422 [00:30<00:00, 233864.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "32768it [00:00, 297726.64it/s]           \u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 24576/1648877 [00:00<00:08, 192415.00it/s]\u001b[A\n",
      "  3%|▎         | 57344/1648877 [00:00<00:07, 200024.24it/s]\u001b[A\n",
      "  6%|▌         | 98304/1648877 [00:00<00:07, 211417.78it/s]\u001b[A\n",
      "  8%|▊         | 131072/1648877 [00:00<00:06, 219018.37it/s]\u001b[A\n",
      " 11%|█         | 180224/1648877 [00:00<00:05, 260502.37it/s]\u001b[A\n",
      " 13%|█▎        | 212992/1648877 [00:00<00:05, 275393.90it/s]\u001b[A\n",
      " 15%|█▍        | 245760/1648877 [00:01<00:05, 252746.69it/s]\u001b[A\n",
      " 18%|█▊        | 294912/1648877 [00:01<00:04, 294798.00it/s]\u001b[A\n",
      " 21%|██        | 344064/1648877 [00:01<00:03, 330778.40it/s]\u001b[A\n",
      " 25%|██▍       | 409600/1648877 [00:01<00:03, 383126.76it/s]\u001b[A\n",
      " 29%|██▉       | 483328/1648877 [00:01<00:02, 408889.87it/s]\u001b[A\n",
      " 34%|███▍      | 557056/1648877 [00:01<00:02, 470526.63it/s]\u001b[A\n",
      " 38%|███▊      | 622592/1648877 [00:01<00:02, 479223.16it/s]\u001b[A\n",
      " 42%|████▏     | 696320/1648877 [00:01<00:01, 535131.07it/s]\u001b[A\n",
      " 46%|████▌     | 761856/1648877 [00:01<00:01, 529382.38it/s]\u001b[A\n",
      " 50%|████▉     | 819200/1648877 [00:02<00:01, 466491.94it/s]\u001b[A\n",
      " 53%|█████▎    | 876544/1648877 [00:02<00:01, 435926.90it/s]\u001b[A\n",
      " 56%|█████▌    | 925696/1648877 [00:02<00:01, 368279.01it/s]\u001b[A\n",
      " 62%|██████▏   | 1024000/1648877 [00:02<00:01, 446389.60it/s]\u001b[A\n",
      " 67%|██████▋   | 1097728/1648877 [00:02<00:01, 496073.71it/s]\u001b[A\n",
      " 71%|███████   | 1163264/1648877 [00:02<00:00, 515034.08it/s]\u001b[A\n",
      " 75%|███████▍  | 1228800/1648877 [00:03<00:01, 265014.84it/s]\u001b[A\n",
      " 78%|███████▊  | 1277952/1648877 [00:03<00:01, 304074.97it/s]\u001b[A\n",
      " 82%|████████▏ | 1351680/1648877 [00:03<00:00, 355925.65it/s]\u001b[A\n",
      " 85%|████████▌ | 1409024/1648877 [00:03<00:00, 368182.07it/s]\u001b[A\n",
      " 88%|████████▊ | 1458176/1648877 [00:03<00:00, 374109.87it/s]\u001b[A\n",
      " 91%|█████████▏| 1507328/1648877 [00:03<00:00, 391529.68it/s]\u001b[A\n",
      " 94%|█████████▍| 1556480/1648877 [00:04<00:00, 396076.17it/s]\u001b[A\n",
      " 98%|█████████▊| 1613824/1648877 [00:04<00:00, 434818.12it/s]\u001b[A\n",
      "1654784it [00:04, 387252.10it/s]                             \u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\n",
      "8192it [00:00, 67674.58it/s]            \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9920512it [00:50, 233864.49it/s]                             "
     ]
    }
   ],
   "source": [
    "# test data is out-of-sample data that your machine have never seen.\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "# specify where you want the data to go, '' means locally\n",
    "# things we want to apply to the data, the data is not natively already in tensor, so we need to convert it to tensor\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "# then, the data in downloaded in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it to another type of object that help us iterate over that data\n",
    "# specify the batch size   \n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch_size__\n",
    "\n",
    "How many data at a time do we want to pass to our model. It also can help us to generalize. Commen batch size is from 8-64. \n",
    "\n",
    "__Training and Testing data split__\n",
    "\n",
    "To train any machine learning model, we want to first off have training and validation datasets. This is so we can use data that the machine has never seen before to \"test\" the machine.\n",
    "\n",
    "__Shuffling__\n",
    "\n",
    "Then, within our training dataset, we generally want to randomly shuffle the input data as much as possible to hopefully not have any patterns in the data that might throw the machine off.\n",
    "\n",
    "For example, if you fed the machine a bunch of images of zeros, the machine would learn to classify everything as zero. Then you'd start feeding it ones, and the machine would figure out pretty quick to classify everything as ones...and so on. Whenever you stop, the machine would probably just classify everything as the last thing you trained on. If you shuffle the data, your machine is much more likely to figure out what's what.\n",
    "\n",
    "__Scaling and normalization__\n",
    "\n",
    "Another consideration at some point in the pipeline is usually scaling/normalization of the dataset. In general, we want all input data to be between zero and one. Often many datasets will contain data in ranges that are not within this range, and we generally will want to come up with a way to scale the data to be within this range.\n",
    "\n",
    "For example, an image is comprised of pixel values, most often in the range of 0 to 255. To scale image data, you usually just divide by 255. That's it. Even though all features are just pixels, and all you do is divide by 255 before passing to the neural network, this makes a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 6, 8, 6, 6, 3, 5, 3, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "# there is ten example of handwriten numbers, and ten tensors of the actual output\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration will contain a batch of 10 elements (that was the batch size we chose), and 10 classes. Let's just look at one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is our input data. The features. The thing we want to predict. y is our label. The classification. The thing we hope the neural network can learn to predict. We can see this by doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.6627,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.7333,\n",
      "          0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.9922,\n",
      "          0.2588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.9922,\n",
      "          0.5490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.9961,\n",
      "          0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.9922,\n",
      "          0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.7765, 0.9922,\n",
      "          0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 0.9922, 0.6941,\n",
      "          0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 0.9961, 0.6627,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 0.9922, 0.6588,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4588, 0.9922, 0.6588,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 0.9922, 0.6588,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5569, 0.9961, 0.6627,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 0.9922, 0.6588,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 0.9922, 0.6588,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 0.9922, 0.6588,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8039, 0.9961, 0.4196,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.2196,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.2196,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5961, 0.0235,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize \n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is data balancing?__\n",
    "\n",
    "Recall before how I explained that if we don't shuffle our data, the machine will learn things like what the last few hundred classes were in a row, and probably just predict that from there on out.\n",
    "\n",
    "Well, with data balancing, a similar thing could occur.\n",
    "\n",
    "Imagine you have a dataset of cats and dogs. 7200 images are dogs, and 1800 are cats. This is quite the imbalance. The classifier is highly likely to find out that it can very quickly and easily get to a 72% accuracy by simple always predicting dog. It is highly unlikely that the model will recover from something like this.\n",
    "\n",
    "Other times, the imbalance isn't quite as severe, but still enough to make the model almost always predict a certain way except in the most obvious-to-it-of cases. Anyway, it's best if we can balance the dataset.\n",
    "\n",
    "By \"balance,\" I mean make sure there are the same number of examples for each classifications in training.\n",
    "\n",
    "Sometimes, this simply isn't possible. There are ways for us to handle for this with special class weighting for the optimizer to take note of, but, even this doesn't always work. Personally, I've never had success with this in any real world application.\n",
    "\n",
    "In our case, how might we confirm the balance of data? Well, we just need to iterate over everything and make a count. Pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "0: 9.871667%\n",
      "1: 11.236667%\n",
      "2: 9.930000%\n",
      "3: 10.218333%\n",
      "4: 9.736667%\n",
      "5: 9.035000%\n",
      "6: 9.863333%\n",
      "7: 10.441667%\n",
      "8: 9.751667%\n",
      "9: 9.915000%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    # data have two set of tensors\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100.0:5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
